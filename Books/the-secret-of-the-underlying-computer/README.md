# 컴퓨터 밑바닥의 비밀

## Chapter 01. 프로그래밍 언어부터 프로그램 실행까지, 이렇게 진행된다

- 프로그래밍 언어는 컴퓨터 과학의 작은 일부분일뿐이다.

### 1.1 여러분이 프로그래밍 언어를 발명한다면?

- CPU는 결국 수많은 스위치의 집합체이다.
  - 스위치는 0과 1의 두 가지 상태를 가진다.

#### 1.1.1 창세기: CPU는 똑똑한 바보

- CPU는 단순 작업에 매우 효과적이고 빠르다.
- 그러나 초기 프로그래밍에서는 별도의 언어가 없어서 CPU의 명령어를 직접 작성해야 했다.
  - 천공카드, 천공테이프, 천공테이프 리더기 등의 기계를 사용했다.
- 예시

    ```binary
    00101001 00000000 00000000 00001000
    00101001 00000000 00000000 00001000
    ```

#### 1.1.2 어셈블리어 등장

- 이후 CPU가 수행하는 것은 결국 단순한 몇 개의 명령어라는 것을 알게 되었다.
  - 가산, 점프, 저장 등의 명령어 등
- 이 명령어를 사람이 이해할 수 있는 언어로 변환한 것이 어셈블리어이다.
- 예시

    ```assembly
    sub $8, %rsp
    mov $.LC0, %edi
    call puts
    mov $0, %eax
    ```

- 이러한 어셈블리어를 사용하면 CPU의 명령어를 직접 작성하는 것보다는 훨씬 편리하다.
- 그러나 이러한 어셈블리어를 바이너리로 변환하는 과정이 필요하다.

#### 1.1.3 저수준 계층의 세부 사항 대 고수준 계층의 추상화

- 어셈블리어는 사람이 인식할 수 있지만 여전히 저수준 언어이다.
  - 결국 작성자가 모든 세부 사항을 신경써야 한다.
- 이러한 어셈블리어를 추상화한 것이 고급 프로그래밍 언어이다.

#### 1.1.4 가득한 규칙: 고급 프로그래밍 언어의 시작

- 어셈블리어로 작성하는 코드에서 특정한 패턴이 발견되었고, 이를 추상화하여 고급 프로그래밍 언어를 만들었다.
  - 조건문, 반복문 및 함수 호출 등의 패턴이 추상화되었다.

#### 1.1.5 <인셉션>과 재귀: 코드 본질

- 조건문, 반복문 등으로 구성된 코드는 여러 번 중첩 될 수 있으며, 이는 결국 재귀로 표현 할 수 있다.
- 이렇게 재귀로 표현되는 최소 단위의 표현을 구문(syntax)이라고 한다.

#### 1.1.6 컴퓨터가 재귀를 이해하도록 만들기

- 위에서 말한 것 처럼 재귀적으로 표현되는 구문을 컴퓨터가 이해할 수 있도록 만들어야 한다.
- 이를 위해 구문을 트리 구조로 변환한다.

#### 1.1.7 우수한 번역가: 컴파일러

- 위에서 말한 여러 과정을 처리하는 프로그램을 컴파일러라고 한다.
  - 컴파일러는 작성된 코드를 파싱하여 트리 구조로 변환하고, 이를 기계어로 변환한다.

#### 1.1.8 해석형 언어의 탄생

- 다양한 CPU의 구조를 고려하여 매번 컴파일하는 것은 매우 번거롭다.
- 이를 해결하기 위해 해석형 언어가 등장했다.
  - 해석형 언어는 표준 명령어라는 중간 단계를 거쳐 CPU에 맞게 변환된다.

#### 정리 및 과제

> 본인이 사용하는 언어가 코드 -> 표준 명령어 -> 인터프리터 -> CPU 과정을 어떤 식으로 수행하는지 알아보자.

- Javascript의 경우
  - 렉싱 -> 파싱 -> AST -> 바이트코드 -> 인터프리터(Ignition, 빠른 실행) -> 인터프리터(TurboFan, 최적화) -> CPU

### 1.2 컴파일러는 어떻게 작동하는 것일까?

- 컴파일러의 사용은 매우 간단하다. 그러나 이 이면에는 많은 과정이 숨어 있다.

#### 1.2.1 컴파일러는 그저 일반적인 프로그램일 뿐, 대단하지 않다.

- 컴파일러
  - 고수준 언어를 저수준 언어로 번역하는 프로그램.
  - 복잡도가 높은 프로그램이지만, 결국 일반적인 프로그램일 뿐이다.

#### 1.2.2 각각의 토큰 추출하기

- 컴파일러가 가장 먼저 하는 일은 소스 코드를 토큰으로 분리하는 것이다.
- 이 과정을 어휘 분석(lexical analysis; lexing)이라고 한다.
  - 예시
    - 원본 코드
     
      ```c
      int a = 1;
      int b = 2;

      while (a < b) {
        b = b - 1;
      }
      ```
    
    - 토큰화된 코드

      ```
      T_Keyword    int
      T_Identifier a
      T_Assign     =
      T_Int        1
      T_Semicolon  ;
      T_Keyword    int
      ...
      ```

#### 1.2.3 토큰이 표현하고자 하는 의미

- 렉싱을 거친 토큰을 이용하여 해석(parsing)을 진행한다.
- 이 과정에서 문법적 오류를 확인하고, 구문 트리(syntax tree)를 생성한다.

#### 1.2.4 생성된 구문 트리에 이상은 없을까?

- 컴파일(해석) 과정에서 문법적 오류를 확인하기에 구문 트리에는 문법적 오류가 없다.
- 그러나 아직 의미적 오류는 확인하지 않았고, 이에 대한 검사가 필요하다.
- 이 과정을 의미 분석(semantic analysis)이라고 한다.
  - 정수에 문자열을 더하거나, 비교의 좌우에 다른 타입의 변수를 사용하는 등의 오류를 확인한다.

#### 1.2.5 구문 트리를 기반으로 중간 코드 생성하기

- 의미 분석을 통과한 구문 트리를 기반으로 중간 코드를 생성한다.
  - 중간 코드(Intermediate Representation; IR)는 CPU에 종속되지 않는 코드이다.
  - 이 과정에서 최적화가 진행되기도 한다.

#### 1.2.6 코드 생성

- 중간 코드를 기반으로 CPU에 맞는 어셈블리어로 변환하고, 다시 이를 기계어로 변환한다.
- 하지만 지금까지의 과정은 하나의 파일에 대한 컴파일이었고, 여러 파일을 컴파일 할 때는 링크(link)라는 과정이 필요하다.

### 1.3 링커의 말할 수 없는 비밀

- 정적 라이브러리와 동적 라이브러리를 사용할 때, 내부적으로 링크 과정이 진행된다.

#### 1.3.1 링커는 이렇게 일한다

- 링커도 컴파일러와 마찬가지로 일반적인 프로그램일 뿐이다.
- 링커는 컴파일러가 만들어낸 여러 파일을 하나로 묶어 하나의 실행 파일을 만든다.
- 이러한 과정은 저자 여러명이 작성한 챕터를 하나로 묶어 하나의 책을 출간하는 것과 유사하다.
  - 도서에서도 특정 장이 다른 장의 내용에 의존하는 경우가 있는데, 이런 종속성을 확인하는 것이 링크의 역할이다.
  - 또한 참조된 페이지가 아직 정확하지 않으므로 컴파일러가 임시로 N쪽이라고 표시한 것을 실제 페이지 번호로 변경하는 것도 링크의 역할이다.

#### 1.3.2 심벌 해석: 수요와 공급

- 심벌(symbol)은 변수나 함수 등의 이름을 의미한다.
- 링커는 이러한 심벌을 해석하여 참조된 심벌이 존재하는지, 하나만 존재하는지를 확인한다.
  - 링커 혼자서는 이러한 정보를 알 수 없고, 컴파일러가 이 정보를 제공한다.
    - 컴파일러가 생성한 대상 파일에는 여러 목적을 가진 영역이 존재한다.
      - 코드 영역: 소스 파일의 함수가 변환된 기계 명령어가 저장되는 부분
      - 데이터 영역: 소스 파일의 전역 변수가 저장되는 부분
      - 심벌 테이블: 공급과 수요, 즉 외부에서 참조 가능한 심벌의 목록과, 참조하고 있는 외부 심벌의 목록이 저장되는 부분
- 즉, 심벌 해석은 대상 파일에서 사용할 외부 심벌을 심벌 테이블에서 찾을 수 있는지 확인하는 과정이다.

#### 1.3.3 정적 라이브러리, 동적 라이브러리, 실행 파일

- 정적 라이브러리(.a, .lib)
  - 의존성을 가진 여러 파일을 각각 컴파일하여 하나의 실행 파일로 만드는 방식
  - 정적 링크(static linking)를 통해 실행 파일을 만든다.
  - 따라서 전체 코드를 매번 컴파일하는 것에 비해 빠르다.
  - 그러나 표준 라이브러리 등을 사용할 때에는 모든 프로그램이 동일한 라이브러리를 사용하므로 중복되는 코드가 많아진다.
    - 그만큼 저장 공간도 낭비하게 되고, 업데이트가 필요할 때 모든 프로그램을 업데이트해야 한다.

- 동적 라이브러리(.so, .dll)
  - 최소한의 정보(이름, 심벌 테이블, 재배치 정보 등)만을 실행 파일에 저장함.
  - 동적 링크(dynamic linking) 과정을 통해 실제 라이브러리를 실행 파일에 연결한다.
  - 동적 링크에는 두 가지 방식이 존재한다.
    - 첫 번째 방식은 프로그램이 메모리에 적재될 때 동적 링크를 수행하는 방식이다.
    - 두 번째 방식은 프로그램의 실행 시간(runtime)에 동적 링크를 수행하는 방식이다.

#### 1.3.4 동적 라이브러리의 장단점

- 장점
  - 동적 라이브러리에 의존하는 프로그램의 개수와 상관없이 동적 라이브러리는 하나만 저장된다.
    - 이런 특징으로 인해 공유 라이브러리(shared library)라고도 불린다.
  - 코드가 수정되는 경우에도 하나의 라이브러리만 업데이트하면 되므로 관리가 편리하다.
  - 플러그인이나 확장 모듈을 만들 때 유용하다.

- 단점
  - 실행 시간에 동적 링크를 수행하므로 정적 링크에 비해 실행 시간이 느리다.
  - 메모리 주소와 독립되어 있어 독립 코드(position-independent code)라고도 불린다.
  - 메모리에 단 하나의 복사본만 존재하고, 이것이 공유되므로 임의의 절대 주소를 참조할 수 없다.
  - 동적 라이브러리를 사용한 경우, 실행파일만으로는 실행이 불가능하다. 별도의 종속성 라이브러리의 설치가 필요하다.

#### 1.3.5 재배치: 심벌의 실행 시 주소 결정하기

- 어셈블리어로 작성된 코드를 보면, 변수 정보가 없고 메모리 주소를 직접 참조한다.
- 링커는 프로그램 실행 시 함수가 적재될 메모리 주소를 결정해야 한다. 그러나 이를 혼자서는 알 수 없다.
- 그렇기에 컴파일 과정에서 컴파일러가 임시로 메모리 주소를 할당한다.
  - 이를 .relo.text, .relo.data 등의 재배치 정보(relocation information)라고 한다.
- 링커는 이 정보를 이용하여 실행 시 메모리 주소를 결정한다. 이 과정을 재배치(relocation)라고 한다.
- 그러나 이 과정에서도 링커가 절대 주소를 알 수 없다. 따라서 가상 메모리(virtual memory)를 사용한다.

#### 1.3.6 가상 메모리와 프로그램 메모리 구조

- 프로그램이 실행된 후 메모리의 상태
  - 커널, 스택, 여유 공간, 힙, 데이터, 코드 등의 영역이 존재한다.
    - 스택: 메모리의 상위 영역에 위치하며, 함수 호출 시 사용되는 지역 변수, 함수의 인자, 함수의 반환 주소 등이 저장된다.
    - 힙: 메모리의 하위 영역에 위치하며, 동적 할당된 메모리가 저장된다.
    - 데이터: 전역 변수, 정적 변수, 문자열 등이 저장된다.
    - 코드: 프로그램 코드가 저장된다.
- 모든 프로그램은 예외 없이, 코드 영역의 시작주소가 `0x400000`이다. 이는 동시에 여러 프로그램을 실행해도 동일하다.
  - 이는 운영체제의 `가상 메모리`를 사용하기 때문으로, 모든 프로그램은 자신이 전체 메모리를 독점하는것 처럼 착각하게 된다.
- 이로인해 링커는 실제 데이터가 물리 메모리의 어디에 위치하는지 알 필요가 없어진다.
- 운영체제는 이러한 가상메모리 관리를 위해 메모리 페이지(memory page) 단위로 매핑한다.
  - 이렇게 기록된 자료를 페이지 테이블(page table)이라고 한다.

- 다시 정리하자면,
  - 모든 프로세스의 가상 메모리는 표준화되어있고, 동일한 크기를 가진다.
  - 가상 메모리의 각 영역의 크기는 다를 수 있어도, 배치되는 순서는 동일하다(스택, 힙, 데이터, 코드).
  - 실제 물리 메모리와 가상 메모리의 크기는 무관하다.
  - 물리 메모리에는 스택, 힙, 데이터, 코드 등이 구분되지 않는다.
  - 모든 프로세스는 자신만의 페이지 테이블을 가진다.

### 1.4 컴퓨터 과학에서 추상화가 중요한 이유

- 사람의 이름도 일종의 추상화다.
  - 이름이 없다면 한 사람을 특정할 때 수많은 부가정보가 필요하다.
  - 그러나 이름을 통해, 간편하고 빠르게, 불필요한 정보의 노출 없이 한 사람을 특정 할 수 있다.
- 이전장에서의 가상 메모리도 추상화의 한 예이다.
  - 프로그램은 자신이 전체 메모리를 독점하는 것처럼 착각하게 되지만, 실제로는 가상 메모리를 사용한다.
  - 이러한 추상화로 인해 프로그램은 메모리의 물리적 위치를 알 필요가 없어진다.

#### 1.4.1 프로그래밍과 추상화

- 프로그래머도 추상화를 통해 많은 이점을 얻을 수 있다.
  - 복잡한 문제를 단순화하여 해결할 수 있다.
  - 원하는 수준으로 프로그램의 복잡도를 제어 할 수 있다.

#### 1.4.2 시스템 설계와 추상화

- 컴퓨터 시스템은 기본적으로 추상화의 연속이다.
  - CPU를 사용할 때, 트랜지스터의 동작 원리를 알 필요가 없다.
    - 몇가지 추상화된 명령어를 사용하여 프로그램을 작성하면 된다.
  - 입출력 장치는 파일로 추상화되어 있다.
  - 실행중인 프로그램은 프로세스로 추상화되어 있다.
  - 물리 메모리는 가상 메모리로 추상화되어 있다.
  - 네트워크는 소켓으로 추상화되어 있다.
  - 프로세스와 프로세스에 종속적인 실행 환경은 컨테이너로 추상화되어 있다.
  - CPU, 운영체제, 응용프로그램은 가상 머신으로 추상화되어 있다.
- 그러나 보다 세밀한 조정을 위해서는 추상화를 넘어 기저 계층을 이해할 필요가 있다.

## Chapter 02. 프로그램이 실행되었지만, 뭐가 뭔지 하나도 모르겠다

- 프로그램은 어떻게 실행될까?
- 실행되고나면 그 모습은 어떻게 변화할까?
- 운영체제와 같은 것들은 왜 필요할까?
- 프로세스, 스레드, 코루틴은 대체 뭘까?
- 콜백함수, 동기화, 비동기화, 블로킹, 논블로킹은 무슨 뜻일까?
- 프로그래머가 이런 개념을 이해해야 하는 이유는 무엇일까?
- 이런 개념에서 프로그래머는 무엇을 할 수 있을까?
- 이런 개념으로 기계의 성능을 최대한 활용하려면 어떻게 해야 할까?

### 2.1 운영 체제, 프로세스, 스레드의 근본 이해하기

#### 2.1.1 모든 것은 CPU에서 시작된다

- CPU에는 혼란스러운 개념이 없고, 모든것이 명확하므로 시작하기에 좋다.
- CPU는 아래의 단 두 가지 사항만 알고있으며, 프로세스나 스레드가 무엇인지 알지 못한다.
  1. 메모리에서 명령어(instruction)를 읽어 가져온다(dispatch).
  2. 가져온 명령어를 실행(execute)하고, 다시 1로 돌아간다.
- CPU는 프로그램 카운터(program counter; PC)라는 레지스터를 가지고 있으며, 이는 다음에 실행할 명령어의 주소를 가리킨다.
  - 레지스터란 CPU 내부에 존재하는 메모리로, 매우 빠르게 접근할 수 있다.
- 그러나 이렇게 순차 접근만 가능하다면 if문이나 반복문을 사용할 수 없다.
  - 이를 위해 점프를 사용하여 PC 레지스터 값을 변경한다.
  
- 프로그램이 생성되고 실행되는 과정
  1. 프로그래머에 의해 소스 코드가 작성된다.
  2. 컴파일러가 이를 기계어(실행 파일)로 변환한다.
  3. 실행 파일이 디스크에 저장된다.
  4. 실행 파일이 메모리에 적재된다.
  5. CPU가 실행 파일을 실행한다.

- 여기에서 프로그래머가 작성한 최초의 함수인 main 함수가 등장하고, 이 함수의 위치가 PC 레지스터에 저장되어 처음으로 실행된다.

#### 2.1.2 CPU에서 운영 체제까지

- CPU로 프로그램을 실행하려면, 위에서 알아본 절차대로 수동으로 진행하면 된다.
  - 이 과정에서 추가로 고려할 사항은 아래와 같다.
    - 프로그램을 적재 할 적절한 크기의 메모리 영역을 찾는다.
    - CPU 레지스터를 초기화하고 함수의 진입 포인트를 찾아 PC 레지스터에 저장한다.
  - 또한 이 경우 아래와 같은 단점을 가진다.
    - 한번에 하나의 프로그램만 실행이 가능하다.
    - 모든 하드웨어를 직접 드라이버와 연결하여 사용해야 한다.
    - 기본적인 라이브러리를 직접 관리해야 한다.

- 위와 같은 문제를 해결하기 위해, 운영 체제가 등장했다.

- 운영체제는 프로그램을 컨텍스트를 가진 프로세스로 추상화하여, 멀티태스킹을 구현한다.
  - 멀티 태스킹은 CPU가 여러 프로세스를 번갈아가며 실행하는 것을 의미한다.

#### 2.1.3 프로세스는 매우 훌륭하지만, 아직 불편하다

- 아래와 같은 코드가 있다고 하자

  ```c
  int main()
  {
    int resA = funcA();
    int resB = funcB();

    print(resA + resB);
    return 0;
  }
  ```

    - 위 코드에서 funcA와 funcB는 서로 독립적으로 실행될 수 있다.
    - 그러나 위 코드는 순차적으로 실행되므로, funcA가 끝나야 funcB가 실행된다.

- 만약 위의 코드를 멀티 프로세스로 실행한다면, funcA와 funcB는 독립적으로 실행될 수 있다.
  - 이러한 멀티 프로세스는 운영체제가 제공하는 기능이다.
  - 이러한 멀티 프로세스는 프로세스 간 통신(IPC)이 필요하다.
    - IPC는 프로세스 간 데이터를 주고 받는 방법을 의미한다.
    - 이는 매우 복잡하고, 오버헤드가 크다.

#### 2.1.4 프로세스에서 스레드로 진화

- 프로세스의 주소 공간에는 CPU가 실행하는 기계 명령어와 함수가 실행되는 스택 정보가 존재한다.
- 이러한 스택 정보를 별도로 관리하여, 프로세스 내에서 독립적으로 실행되는 스레드를 만들 수 있다.
  - 이러한 스레드는 프로세스 주소 공간을 공유하므로, IPC가 필요 없다.
- 따라서 위의 코드를 스레드로 실행하면, funcA와 funcB가 동시에 실행되게 할 수 있고, IPC가 필요 없다.
- 이런 이유로 스레드를 경량 프로세스라고도 부른다.
- 물론 CPU에 멀티 코어가 있어야만 다중 스레드가 가능한 것은 아니다.
  - 앞서 말했듯, CPU는 스레드와 프로세스를 모른다.

- 그러나 스레드에도 단점은 존재한다.
  - 앞서 말했듯 주소 공간을 공유하므로, 스레드 간 리소스 공유에 주의해야 한다.
  - 따라서 프로그래머는 상호 배제(mutual exclusion)와 동기화(synchronization)를 고려해야 한다.

#### 2.1.5 다중 스레드와 메모리 구조

- 스레드는 프로세스의 주소 공간을 공유하므로, 스레드 간 데이터 공유가 가능하다.
- 그러나 각 스레드는 독립적인 스택을 가진다.
  - 즉 주소 공간에 독립적인 스택이 생성되므로, 스레드를 만들면 메모리의 공간이 소모된다.

#### 2.1.6 스레드 활용 예

- 스레드가 처리하는 작업의 종류
  - 긴 작업(long task): 파일을 읽고 쓰는 작업
  - 짧은 작업(short task): 짧은 네트워크 요청, 데이터베이스 쿼리 등
    - 하나의 작업은 짧으나 매우 많은 요청이 들어온다.
      - 이런 경우 매 요청당 스레드를 생성하면(요청당 스레드; thread per request) 큰 오버헤드가 발생한다.
        - 스레드의 생성과 종료에 자원이 소비된다.
        - 스레드마다 독립된 스택을 가지므로, 메모리 소비가 크다.
        - 스레드 전환에도 오버헤드가 발생한다.

- 위와 같은 이유로 스레드 풀(thread pool)이 등장했다.

#### 2.1.7 스레드 풀의 동작 방식

- 스레드 풀의 개념은 매우 간단하다.
  - 미리 스레드를 생성해 놓고, 요청이 들어오면 이를 할당하는 방식이다.
  - 이를 통해 스레드 생성과 종료에 따른 오버헤드를 줄일 수 있다.
- 이 과정에서 스레드 풀은 큐를 사용한다.
  - 생산자-소비자 패턴을 사용한다.
  - 요청이 들어오면 큐에 저장하고, 스레드 풀에서 스레드를 할당하여 처리한다.
  - 이를 통해 요청이 많아져도 스레드 풀의 스레드 수만큼만 처리하면 되므로, 오버헤드를 줄일 수 있다.

- 스레드 풀에 전달되는 작업은 두 가지 사항을 포함한다.

  ```c
  struct Task
  {
    void* data;
    handler handle;
  }
  ```

    - 처리할 데이터
    - 처리할 함수

- 스레드 풀의 동작 방식
  1. 스레드 풀이 생성되면, 스레드를 생성하고 큐를 생성한다.
    - 처음엔 스레드 풀의 스레드는 블로킹 상태로 대기한다.
  2. 스레드 풀에 작업이 들어오면, 큐에 저장한다.
  3. 스레드 풀의 스레드는 큐에서 작업을 가져와 처리한다.
    - 앞서 알아본 구조체의 handle 함수를 호출하여 data를 처리한다.
    - 이 때 큐에 대해 상호배제(mutual exclusion)를 반드시 처리해야 한다.
  4. 작업이 완료되면, 결과를 반환하고 다음 작업을 처리한다.

- 예시 코드

  ```c
  while (true)
  {
    struct task = GetFromQueue(); // 큐에서 작업을 가져온다.

    task->handle(task->data); // 작업을 처리한다.
  }
  ```

#### 2.1.8 스레드 풀의 스레드 수

- 스레드 풀의 스레드 수는 중요한 요소이다.
  - 반면 스레드 풀의 스레드 수가 너무 적으면, CPU를 활용하지 못하게 된다.
  - 스레드 풀의 스레드 수가 너무 많으면, 메모리 소비가 커지고, 스레드 전환에 오버헤드가 발생한다.
- 판단의 근거는 작업의 종류와 시간이다.
  - CPU 바운드 작업: CPU를 많이 사용하는 작업
    - CPU 바운드 작업은 CPU 코어 수만큼 스레드를 생성하는 것이 좋다.
  - I/O 바운드 작업: I/O를 많이 사용하는 작업
    - I/O 바운드 작업은 N * (1 + 대기시간 / 처리시간)의 공식을 사용한다.
      - 대기시간과 처리시간이 비슷하다면, 스레드 수는 `CPU 코어 수 * 2`와 비슷하다.
- 그러나 위의 수치는 이론적인 수치이며, 실제로는 테스트를 통해 적절한 수치를 찾아야 한다.

### 2.2 스레드 간 공유되는 프로세스 리소스

- 프로세스: 운영체제가 리소스를 할당하는 기본 단위
- 스레드: 프로세스 내에 존재하는 실행 단위. 스케줄링의 기본 단위
  - 따라서 프로세스의 리소스는 스레드 간 공유된다.
  - 그러나 스레드간 공유되지 않는 리소스(스레드 전용)도 존재한다.

#### 2.2.1 스레드 전용 리소스

- 상태변화의 관점에서 스레드는 사실 함수 실행일 뿐임.
  - 기본적인 main 함수의 실행과 다를 바 없는 실행 흐름에 스레드라는 이름을 붙인 것임.
- 함수의 실행 정보 저장을 위해 `스택 프레임`(3장 참조)을 사용함.
  - 함수의 반환값, 매개변수, 지역변수 및 레지스터 정보 등을 저장함.
- 이러한 스택 프레임은 프로세스의 주소 공간 중 스택 영역에 저장됨
  - 따라서 스레드가 여러개라면 스택 영역도 여러 개 존재함.
- 결론적으로 프로세스의 스택 영역에 속한 정보(스레드 컨텍스트)는 스레드 전용이라는 것을 알 수 있음.
  - 스레드 영역을 제외한 나머지 영역은 스레드 간 공유됨.

#### 2.2.2 코드 영역: 모든 함수를 스레드에 배치하여 실행할 수 있다

- 프로세스 주소 공간 중 코드 영역에는 프로그램 코드가 저장됨.
  - 이 코드는 CPU가 실행하는 기계어로 변환된 명령어들임.
  - 이 정보는 프로그램 실행 시 메모리에 적재됨.
- 이러한 코드 영역은 모든 스레드가 공유함.
  - 따라서 모든 스레드는 동일한 프로그램 코드를 실행함.
- 그러나 코드 영역은 읽기 전용이므로, 스레드 세이프(thread safe)함.
  - 즉, 동시에 여러 스레드가 코드 영역에 접근해도 문제가 발생하지 않음.

#### 2.2.3 데이터 영역: 모든 스레드가 데이터 영역의 변수에 접근할 수 있다

- 프로세스 주소 공간 중 데이터 영역에는 전역 변수, 정적 변수, 문자열 등이 저장됨.
  - 이러한 데이터 영역은 프로세스의 시작과 함께 생성되며, 프로세스의 종료와 함께 소멸됨.
- 데이터 영역도 코드 영역과 마찬가지로 모든 스레드가 공유함.
  - 따라서 모든 스레드는 동일한 데이터 영역의 변수에 접근할 수 있음.
- 그러나 데이터 영역은 읽기/쓰기가 가능하므로, 스레드 세이프하지 않음.
  - 따라서 스레드 간 데이터 공유 시 주의가 필요함.

#### 2.2.4 힙 영역: 포인터가 핵심이다

- 프로세스 주소 공간 중 힙 영역에는 동적 할당된 메모리가 저장됨.
  - C/C++의 `malloc`, `free`, `new`, `delete` 등의 함수를 사용하여 메모리를 할당하고 해제함.
- 힙 영역도 코드 영역과 마찬가지로 모든 스레드가 공유함.

#### 2.2.5 스택 영역: 공유 공간 내 전용 데이터

- 프로세스 주소 공간 중 스택 영역에는 함수의 실행 정보가 저장됨.
- 이러한 스택 영역은 스레드 전용임.
- 그러나 엄밀히 말하면, 스택 영역은 스레드 전용이라고 할 수 없음.
  - 다른 스레드의 스택 영역의 메모리 주소를 안다면, 다른 스레드의 스택 영역에 접근할 수 있음.
  - 즉, 프로세스처럼 완전히 격리되어 접근할 수 없는 것은 아님.
- 이러한 느슨한 격리방식으로 인해 스택 영역을 편리하게 사용할 수 있지만, 그만큼 주의가 필요함.

#### 2.2.6 동적 링크 라이브러리와 파일

- 앞서 동적 라이브러리의 코드와 데이터는 프로그램 실행 시 메모리에 적재된다고 하였음.
- 이러한 동적 라이브러리는 프로세스 주소 공간의 여유 공간에 적재됨.
  - 스택 영역과 힙 영역의 사이의 여유공간에 적재됨.
  - 즉, 모든 스레드가 해당 자원을 공유함

#### 2.2.7 스레드 전용 저장소

- 스레드 전용 저장소(thread local storage; TLS)는 스레드 전용 데이터를 저장하는 공간임.
  - 이는 스레드 전용 데이터를 저장하기 위한 전역 변수와 같은 역할을 함.
  - 모든 스레드에서 변수에 접근 할 수 있음.
  - 그러나 이 변수의 인스턴스는 스레드마다 별도로 생성되어, 변경시에도 다른 스레드에 영향을 주지 않음.

- 코드 예시
  - 일반적인 전역 변수

    ```cpp
    int a = 1; // global variable

    void print_a()
    {
      cout << a << endl;
    }

    void run()
    {
      ++a;
      print_a();
    }

    void main()
    {
      thread t1(run);
      t1.join(); // 2

      thread t2(run);
      t2.join(); // 3
    }
    ```
  
  - TLS를 사용한 전역 변수

    ```cpp
    __thread int a = 1; // TLS variable

    // ...

    // 출력 결과
    // 2
    // 2
    ```

      - `__thread` 키워드를 사용하여 TLS 변수를 선언함.
      - 이 변수는 스레드마다 별도로 생성되어, 다른 스레드에 영향을 주지 않음.

### 2.3 스레드 세이프 코드는 도대체 어떻게 작성해야 할까?

#### 2.3.1 자유와 제약

- 스레드 세이프 코드는 공공장소를 예를 들어 설명할 수 있다.
- 스레드가 자신만의 데이터를 사용한다면, 이는 당연히 스레드 세이프하다.
- 그러나 스레드가 공유 데이터를 사용한다면, 공공시설을 사용하는 것 처럼 주의가 필요하다.
  - 다른 스레드의 공유 리소스 사용 순서를 방해하지 않아야 한다.

#### 2.3.2 스레드 세이프란 무엇일까?

- 스레드 세이프
  - 여러 스레드가 동시에 호출되어도, 프로그램의 동작이 올바르게 보장되는 것을 의미한다.

#### 2.3.3 스레드 전용 리소스와 공유 리소스

- 스레드 전용 리소스
  - 함수의 지역 변수
  - 스레드의 스택 영역
  - 스레드 전용 저장소
- 공유 리소스: 각종 락, 세마포어 등을 사용해 동기화가 필요한 리소스
  - 힙 영역
  - 데이터 영역
  - 코드 영역: 읽기전용이므로 고려하지 않아도 된다.

#### 2.3.4 스레드 전용 리소스만 사용하기

- 지역 변수만을 사용한 스레드 세이프 코드

  ```cpp
  int func()
  {
    int a = 1;
    int b = 1;

    return a + b;
  }
  ```

#### 2.3.5 스레드 전용 리소스와 함수 매개변수

- call by value 인 경우 스레드 세이프하다.

  ```cpp
  int func(int num)
  {
    num++;
    
    return num;
  }
  ```

- 아래처럼 포인터를 사용한 경우 주의가 필요하다.

  ```cpp
  int global_num = 1;

  int func(int* num)
  {
    ++(*num);

    return *num;
  }

  void thread1()
  {
    func(&global_num);
  }

  void thread2()
  {
    func(&global_num);
  }
  ```

    - 위 코드는 스레드 세이프하지 않다.
    - 이런 함수를 사용하는 경우 매개변수로 해당 스레드에 속하는 리소스를 사용해야 스레드 세이프하다.

#### 2.3.6 전역 변수 사용

- 전역 변수를 읽기전용으로 사용하는 경우 스레드 세이프하다.

  ```cpp
  int global_num = 1;

  int func()
  {
    return global_num;
  }
  ```

- 만약 쓰기 작업이 필요한 경우, 락 등을 사용하여 원자성을 보장해야 한다.

#### 2.3.7 스레드 전용 저장소

- TLS를 사용한 스레드 세이프 코드

  ```cpp
  __thread int global_num = 1;

  int func()
  {
    ++global_num;

    return global_num;
  }
  ```

#### 2.3.8 함수 반환값

- 함수가 값을 반환하는 경우(return by value)

  ```cpp
  int func()
  {
    int a = 100;
    return a;
  }
  ```

- 함수가 포인터를 반환하는 경우(return by reference)

  ```cpp
  int* func()
  {
    static int a = 100;

    return &a;
  }
  ```

#### 2.3.9 스레드 세이프가 아닌 코드 호출하기

- 스레드 세이프하지 않은 코드1

  ```cpp
  int global_num = 0;

  int func()
  {
    ++global_num;

    return global_num;
  }
  ```

- 스레드 세이프한 코드1

  ```cpp
  int funcA()
  {
    mutex l;

    l.lock()
    func();
    l.unlock();
  }
  ```

- 스레드 세이프하지 않은 코드2

  ```cpp
  int func(int * num)
  {
    ++(*num);

    return *num;
  }
  ```

- 스레드 세이프한 코드2: 위 함수를 스레드 세이프하게 호출하는 방법

  ```cpp
  void funcA()
  {
    int a = 100;
    int b = func(&a);
  }
  ```

#### 2.3.10 스레드 세이프 코드는 어떻게 구현할까?

- 다중 스레드 프로그래밍중 최대한 리소스를 공유하지 않는다.
- 스레드 전용 리소스와 스레드 공유 리소스를 구분한다.
- 기억 해 둘 처방전
  - 스레드 전용 저장소(Thread Local Storage; TLS)
    - 전역 변수가 필요한 경우, 스레드 전용 저장소로 사용 가능한지 먼저 고려한다.
  - 읽기 전용(Read Only)
    - 해당 리소스를 읽기 전용으로 사용해도 되는지 먼저 고려한다.
  - 원자성 연산(Atomic Operation)
  - 동기화 시 상호 배제(Mutual Exclusion in Synchronization)
    - 뮤텍스(Mutex), 세마포어(Semaphore), 스핀락(Spinlock) 등을 사용하여 상호 배제를 구현한다.

### 2.4 프로그래머는 코루틴을 어떻게 이해해야 할까?

- 코루틴이란, 함수의 실행을 중간에 멈추고 나중에 이어서 실행할 수 있는 기능을 제공하는 것.

#### 2.4.1 일반 함수

- 파이썬으로 구현한 일반적인 함수

  ```python
  def func():
    print('A')
    print('B')
    print('C')

  def foo():
    func();
  ```

    - foo()는 func()를 호출하고, func()는 순차적으로 실행되고 다시 foo()로 제어가 돌아온다.
    - 즉, 순차적으로 실행된다.
    - 이러한 함수를 `서브루틴(subroutine)`이라고 한다.

#### 2.4.2 일반 함수에서 코루틴으로

- 코루틴은 일반적인 함수와 같지만, 일시중지를 할 수 있다.

  ```python
  def func():
    print('A')
    yield
    print('B')
    yield
    print('C')

  def foo():
    co = func()
    next(co)
    next(co)
    print('in function foo')
    next(co)
  ```

    - 위 코드에서 `yield`는 일시중지를 의미한다.
    - `next(f)`는 다음 `yield`로 이동한다.
    - 즉, `func()`는 `A`, `B`, `C`를 순차적으로 출력하지 않고, `A`, `B`, `in function foo`, `C`를 각각 출력하고 매 번 일시중지한다.

#### 2.4.3 직관적인 코루틴 설명

- 일반적인 함수

  ![일반적인 함수](/images/normal-function.jpeg)

- 코루틴

  ![코루틴](/images/coroutine-function.jpeg)

#### 2.4.4 함수는 그저 코루틴의 특별한 예에 불과하다

- 코루틴은 자신이 일시중지 될 때, 현재의 실행 상태를 저장했다가 나중에 이어서 실행할 수 있는 함수이다.
- 이는 운영체제가 스레드를 스케줄링하는 것과 비슷하다.
- 코루틴은 유저 모드에서 실행되는 스레드라고 볼 수 있다.
- 코루틴이 몇개가 생성되건 운영체제는 이를 알지 못한다.

#### 2.4.5 코루틴의 역사

- 코루틴은 스레드보다 이전에 등장했다.
- 스레드가 존재하지 않았기에, 동시 작업을 위한 개념으로 코루틴이 등장했다.
- 그러나 관심을 받지 못하다가, 최근 서버의 처리량을 높이기 위해 다시 주목받고 있다.

#### 2.4.6 코루틴은 어떻게 구현될까?

- 스레드의 구현과 본질적으로 차이가 없다.

- 스레드의 구현
  - 일시중지 될 때의 상태를 저장하고, 다시 실행될 때 이를 복원한다.
    - 스택 프레임에 CPU 레지스터 정보, 함수 실행 시 상태정보 등을 저장한다.
      ![스택 프레임](/images/stack-frame.jpeg)

- 코루틴의 구현
  - 코루틴은 스레드와 마찬가지로 일시중지 될 때의 상태를 저장하고, 다시 실행될 때 이를 복원한다.
    - 힙 영역에 CPU 레지스터 정보, 함수 실행 시 상태정보 등을 저장한다.
      ![힙 영역](/images/coroutine.jpeg)
        - 해당 주소공간에는 스레드 한개와 코루틴 두개가 존재한다.
        - 총 세 개의 실행 흐름이 존재한다.

### 2.5 콜백 함수를 철저하게 이해한다.

#### 2.5.1 모든 것은 다음 요구에서 시작된다

- A가 B에 의존하는데 모든 구현을 B가 맡는다면 A의 요구사항이 변경될 때 B도 변경되어야 한다.
  - 그러나 이러한 요구사항이 많아지게되면 복잡도가 증가한다.

- 예시

  ```cpp
  void make_donut()
  {
    ...
    if(TeamA)
    {
      form_A();
    }
    else if(TeamC)
    {
      form_C();
    }
    ...
  }
  ```

#### 2.5.2 콜백이 필요한 이유

- 프로그래머는 이미 변수를 많이 사용한다.
  - 이를 통해 값이 변경되는 경우 모든 값을 찾아서 변경할 필요 없이, 변수의 값을 일괄적으로 변경할 수 있다.
  - 이와 비슷하게, 함수도 변수처럼 이용 가능하다.

- 예시

  ```cpp
  void make_donut(func f)
  {
    ...
    f();
    ...
  }

  make_donut(form_A);
  ```

- 이와 같은 함수를 콜백 함수라고 한다.

#### 2.5.3 비동기 콜백

- 동기 코드

  ```cpp
  ...
  make_donut(form_D); // 오래 걸리는 작업
  something_important(); // 중요한 작업이지만 지체되고있다
  ```

- 비동기 코드

  ```cpp
  void real_make_donut()
  {
    ...
    f();
    ...
  }

  void make_donut(func f)
  {
    thread t(real_make_donut, f);
  }
  ```

    - 이렇게 사용하면 `make_donut()` 호출시 새로운 스레드가 생성되어, `form_D()`가 비동기적으로 실행된다.
    - 주의사항으로, `something_important()` 실행 시점에서 `form_D()`가 완료되지 않았을 수 있다.

#### 2.5.4 비동기 콜백은 새로운 프로그래밍 사고방식으로 이어진다

- 일반적으로 사람들은 동기적 사고방식에 익숙하다.
  - 그러나 콜백을 통해 비동기적 프로그래밍이 가능해졌고, 이는 새로운 프로그래밍 사고방식을 만들어냈다.
- 동기적 사고방식에서는 언제 어떤 일을 해야하는지 정확히 예측해서 일 할 수 있다.
- 비동기적 사고방식에서는 어떤 일을 해야한는지는 알고있지만, 언제 일을 해야하는지는 알 수 없다.
  - 언제 하게될지는 콜백을 전달받은 함수가 결정한다.

#### 2.5.5 콜백 함수의 정의

- 콜백함수란, 다른 코드에 매개변수로 전달되는 실행 가능한 코드이다.
- 특히 이런 콜백은 서드파티 라이브러리에서 많이 사용된다.
  - 이를 통해 서드파티 라이브러리의 사용자는 작성자의 구현 코드를 알 필요 없이, 콜백 함수만 전달하면 된다.
  - 사용자는 콜백 함수를 작성할 책임만 있고, 직접 호출하지는 않는다.

#### 2.5.6 두 가지 콜백 유형

- 동기 콜백
  - 콜백 함수가 호출되면, 호출자는 콜백 함수의 실행이 끝날 때까지 기다린다.
- 비동기 콜백
  - 콜백 함수가 호출되면, 호출자는 콜백 함수의 실행이 끝날 때까지 기다리지 않는다.
  - 지연 콜백(deferred callback)이라고도 한다.
  - 다중 코어 환경에서 더 효율적이다.

#### 2.5.7 비동기 콜백의 문제: 콜백 지옥

- 비동기 콜백의 가장 큰 단점은, 이해하기 어렵다는 것이다.
- 특히 비즈니스 로직이 복잡한 경우, 콜백 함수가 중첩되어 복잡해지는 경우가 있다.
  - 이를 `콜백 지옥(callback hell)`이라고 한다.

- 동기 코드 예시

  ```cpp
  a = GetServiceA();
  b = GetServiceB(a);
  c = GetServiceC(b);
  d = GetServiceD(c);
  ```

- 비동기 코드 예시

  ```cpp
  GetServiceA(function(a)
    {
      GetServiceB(a, function(b)
        {
          GetServiceC(b, function(c)
            {
              GetServiceD(c, function(d)
                {
                  ...
                }
              );
            }
          );
        }
      );
    }
  )
  ```

### 2.6 동기와 비동기를 철저하게 이해한다

#### 2.6.1 고된 프로그래머

- 동기와 관련된 키워드: 종속성, 연관성, 기다림
- 비동기와 관련된 키워드: 독립, 무관, 기다리지 않음, 동시성

#### 2.6.2 전화 통화와 이메일 보내기

- 전화통화: 동기
- 이메일 보내기: 비동기

#### 2.6.3 동기 호출

- 예시

  ```cpp
  funcA()
  {
    funcB(); // funcB가 끝날 때까지 기다린다.
    // ...
  }
  ```

    - 일반적으로 `funcA`와` funcB`는 동일한 스레드에서 실행된다.
    - 프로그래머가 이해하기 쉽다.
    - 연산의 효율이 떨어진다.

#### 2.6.4 비동기 호출

- 비동기 호출의 두 가지 상황
  1. 호출자가 실행 결과를 신경쓰지 않는 경우
  2. 호출자가 실행 결과를 반드시 알아야하는 경우

- 호출자가 결과를 신경쓰지 않는 경우

  ```cpp
  void handler(void* buf)
  {
    // ...
  }

  read(buf, handler);
  ```

    - 호출자가 결과를 신경쓰지 않는 경우, `handler`를 통해 처리방법을 함꼐 전달하여 처리를 위임한다.

- 호출자가 결과를 반드시 알아야하는 경우

  ```cpp
  void handler(void* buf)
  {
    // ...
    notify();
  }

  read(buf);
  ```

    - 이 경우 호출자는 `notify()`를 통해 결과를 알 수 있다.
    - 결과를 알게된 호출자가 별도의 handler를 통해 결과를 처리한다.

#### 2.6.5 웹 서버에서 동기와 비동기 작업

- 가장 많은 형태의 비동기 요청은 데이터베이스 요청임.
- 일반적인 동기 처리의 예시

  ```cpp
  // 메인 스레드
  main_thread()
  {
    while (1)
    {
      요청 수신;
      A;
      B;
      C;
      데이터베이스 요청 및 결과대기;
      D;
      E;
      F;
      결과 반환;
    }
  }

  // 데이터베이스 스레드
  database_thread()
  {
    while (1)
    {
      요청 수신;
      데이터베이스 처리;
      결과 반환;
    }
  }
  ```

    - 위 경우 주 스레드에 대기 시간이 발생한다.

- 위 예시를 아래와 같이 비동기로 개선 할 수 있다.
  - 주 스레드가 데이터베이스 스레드의 처리 결과를 알 필요가 없을 떄
    - A, B, C를 주 스레드가 처리한 후, DB 작업과 이어지는 D, E, F는 데이터베이스 스레드에서 처리한다.
    - 이때 D, E, F함수의 실행을 위해 콜백함수를 전달한다.
      - 이후 처리 작업을 DB 스레드에 정의하지 않고 콜백을 사용하는 이유는 DB 스레드가 주 스레드에 종속되지 않기 위함이다.  

  - 주 스레드가 데이터베이스의 작업 결과를 알아야 할 때
    - 주 스레드는 작업 전달 수 이어지는 다른 작업(A2)을 처리한다.
    - DB 스레드가 처리한 결과를 주 스레드에 알린다.
    - 이후 주 스레드는 받은 결과를 가지고 후속 처리(D1, E1, F1)를 진행한다.

  - 다만 모든 경우에 비동기 처리가 무조건적으로 좋은 것은 아니다.
    - 동기 처리가 더 효율적인 경우도 있다.
    - 따라서 상황에 맞게 적절한 처리 방식을 선택해야 한다.

### 2.7 아 맞다! 블로킹과 논블로킹도 있다

#### 2.7.1 블로킹과 논블로킹

- 함수 A가 함수 B를 호출할 때, 운영체제가 함수 A를 실행중인 스레드나 프로세스를 중지시킨다면 함수 A는 블로킹되었다고 한다.
- 즉, 블로킹은 스레드 또는 프로세스가 일시중지되는것을 의미한다.

#### 2.7.2 블로킹의 핵심 문제: 입출력

- 일반적으로 블로킹은 I/O와 밀접하게 관련되어있다.
  - I/O 작업은 CPU 연산(GHz 단위)에 상대적으로 매우 느리기 때문이다.

#### 2.7.3 논블로킹과 비동기 입출력

- 논블로킹 함수를 사용하는 경우 호출 스레드가 정지되지 않고 recv함수를 즉시 반환한다.
- 이후 호출 스레드는 다른 작업을 수행한다.
- 데이터 수신 작업은 커널이 처리한다.

- 작업의 종료를 알 수 있는 세 가지 방법
  1. recv함수 외에 결과를 확인 할 수 있는 함수를 별도로 제공하고 이를 호출하여 결과를 확인한다.
  2. 데이터가 수신되면 스레드에 메시지나 신호를 전달한다.
  3. recv함수 호출 시 데이터 수신처리를 위한 콜백함수를 전달한다.

#### 2.7.4 피자 주문에 비유하기

- 크게 두 가지 경우로 나뉜다
  - 인내심이 강한 경우
    - 배송이 도착하면 알림을 받을 수 있으니 주문 이후 신경을 쓰지 않는다.
    - 그 시간에 다른 일을 처리한다.

  - 인내심이 부족한 경우
    - 매 5분마다 전화를 걸어 배송 상태를 확인한다.
    - 물론 이 경우에도 다른 일을 처리할 수 있지만, 효율적이지는 않다.
    - 확인의 간격이 짧아지거나, 다른 일을 처리할 수 없는 경우 사실상 동기식 처리가 된다.
    - 즉, 논블로킹이 반드시 비동기를 의미하는 것은 아니다.

#### 2.7.5 동기와 블로킹

- 모든 동기 호출이 블로킹인것은 아니다.
  - 간단한 가산 함수 등은 동기 호출이지만 블로킹되지 않는다.
- 블로킹 방식으로 호출된 경우, 이는 동기 호출이다.
  - 호출 스레드가 멈추게 되므로, 결과를 기다려야한다.

#### 2.7.6 비동기와 논블로킹

- 데이터를 수신하는 recv함수를 NON_BLOCKING_FLAG를 통해 논블로킹으로 호출한다.

  ```cpp
  void handler(void *buf)
  {
    // ...
  }

  while (true)
  {
    fd = accept();
    recv(fd, buf, NON_BLOCKING_FLAG, handler); // 논블로킹 호출
  }
  ```

    - 위 코드는 비동기이고, 논블로킹이다.

- 또다른 예시

  ```cpp
  void handler(void *buf)
  {
    // ...
  }

  while (true)
  {
    fd = accept();
    recv(fd, buf, NON_BLOCKING_FLAG); // 호출 후 바로 반환. 논블로킹

    while (!check(fd))
    {
      // 무한루프를 돌며 수신결과를 확인
    }
  }

  handler(buf);
  ```

    - 호출 자체는 비동기 호출이지만, while문을 통해 강제로 블로킹을 걸어놓은 경우이다.
    - 결과적으로 사실상 동기 호출이 된다.
    - 즉 논블로킹이라고 무조건 비동기를 의미하는 것은 아니다.
